# **Experimentation & Model Lifecycle – Onboarding Specification**

**Purpose**  
This document provides a complete foundation for senior developers to work effectively with the experimentation and model-iteration system. It explains the architecture, data model, experiment assignment, event logging, datasets, training, fine-tuning, evaluation, and promotion lifecycles.  

It does not assume prior MLOps experience.

**Core Stack**  
- **PostgreSQL** – primary metadata store (experiments, events, policies)  
- **MLflow** – model registry and experiment tracking  
- **Metabase** – dashboards and experiment visualisation  
- **Open-source LLMs** – LLaMA, Mistral, Qwen, etc.  
- **Object Storage** – S3/MinIO for datasets and ML artefacts  
- **Orchestrator** – Airflow or Prefect  
- **Task Queue** – Redis/worker for asynchronous jobs  
- **Containerisation** – Docker  

---

# **1. System Overview**

The system provides three core capabilities:

1. **Experimentation Layer**  
   - Controls which planner/policy/model variant each user or household receives.  
   - Stores experiment configurations, variant allocation rules, and assignments.  
   - Ensures deterministic assignment and consistent behaviour.

2. **Event Logging & Metrics Layer**  
   - Records plan generation, user interactions, performance metrics, and variant context.  
   - Serves as the foundation for offline datasets and analytics.

3. **Model Training, Fine-Tuning, and Evaluation Workflow**  
   - Generates training datasets from logs.  
   - Runs model training with MLflow.  
   - Performs offline replay evaluation.  
   - Promotes model versions into controlled experiments, then production.

All components are built to be understandable by developers without ML backgrounds while supporting iterative model development.

---

# **2. Repository Structure (Template)**

A structured repo enables developers to navigate the system intuitively.

```
/experimentation-template
  README.md
  /docs
    architecture.md
    experiments.md
    data-model.md
    assignment-service.md
    event-ingestion-service.md
    mlflow-guide.md
    training-workflow.md
    offline-evaluation.md
    model-promotion.md
    mlops-concepts.md
  /services
    /assignment-service
      DESIGN.md
      API_SPEC.md
    /event-ingestion-service
      DESIGN.md
      API_SPEC.md
  /infra
    postgres-schema-overview.sql
    mlflow-setup.md
    metabase-setup.md
    environments.md
  /pipelines
    training-data.md
    metrics-aggregation.md
    offline-replay.md
  /config
    experiments.example.yml
    policies.example.yml
  /analytics
    metabase-models.md
    example-queries.sql
```

Each directory contains a **DESIGN or SPEC** doc, not implementation code.

---

# **3. Architecture Overview**

Core components and their relationships:

```
Client
  ↓
Backend
  → Experiment Assignment Service (EAS)
  → Policy + Model Selection Layer
  → Planner Execution
  → Event Ingestion Service (EIS)
       ↓
       PostgreSQL (events, assignments, experiments)
              ↓
            Metabase (dashboards)
              ↑
       Offline Pipelines (Airflow/Prefect)
              ↓
         MLflow (model registry + tracking)
              ↓
     Datasets in S3/MinIO (training data)
```

Key responsibilities:

- **EAS**: variant assignment and config delivery  
- **EIS**: event recording  
- **Postgres**: metadata + logs  
- **MLflow**: model versions + training runs  
- **Metabase**: visibility and decision support  
- **Pipelines**: dataset generation + evaluation  

---

# **4. Data Model (PostgreSQL)**

Use schema: `exp`.

## **4.1 Experiments**

`exp.experiments`  
- Experiment definition (id, name, status, unit_type, start/end)

`exp.variants`  
- Variants per experiment  
- Each variant includes `allocation` and a `config` (JSON)

`exp.assignments`  
- Mapping of `unit_id` → variant  
- Deterministic  
- One row per `(unit_id, experiment_id)`

## **4.2 Events**

`exp.events`  
- Atomic logs of in-product actions  
- Includes: experiment IDs, variant IDs, metrics, context  

`exp.metric_aggregates`  
- Hourly/daily summaries generated by pipelines  
- Source for Metabase dashboards  

## **4.3 Policy & Model Registry Linking**

`exp.policies`  
- Named policies (e.g. planner policy)

`exp.policy_versions`  
- Versioned references to MLflow model versions  
- Example fields:  
  - `mlflow_model_name`  
  - `mlflow_model_version`  
  - `config_defaults`  

---

# **5. Experiment Assignment Service (EAS)**

The service assigns variants deterministically.

### **API Spec (Conceptual)**

**Request**

```json
{
  "unit_type": "user",
  "unit_id": "user-123",
  "context": { "app_version": "1.2.3" },
  "requested_experiments": ["planner_policy_exp"]
}
```

**Response**

```json
{
  "assignments": [
    {
      "experiment_id": "uuid-exp",
      "variant_id": "uuid-var",
      "variant_name": "variant_a",
      "config": {
        "policy_version_id": "uuid-policy-version",
        "params": { "exploration_rate": 0.15 }
      }
    }
  ]
}
```

### **Assignment Logic (Pseudo-code)**

```pseudo
unit_hash = hash(experiment_id + ":" + unit_id)
bucket = unit_hash mod 10000

for variant in variants:
    cumulative += variant.allocation * 10000
    if bucket < cumulative:
         return variant
```

Assignment is:

- deterministic  
- persisted to `exp.assignments`  
- used on all future requests  

---

# **6. Event Ingestion Service (EIS)**

Captures logs of planner activity, UX behaviour, and system metrics.

### **API Spec (Conceptual)**

**Request**

```json
{
  "event_type": "plan_generated",
  "unit_type": "user",
  "unit_id": "user-123",
  "experiments": [{ "experiment_id": "uuid-exp", "variant_id": "uuid-var" }],
  "context": { "policy_version_id": "uuid-policy-version" },
  "metrics": { "latency_ms": 450 },
  "timestamp": "2025-01-01T10:00:00Z"
}
```

### **Responsibilities**

- Validate input  
- Append to `exp.events`  
- Write to object storage for long-term retention  

---

# **7. Offline Pipelines**

Requires Airflow or Prefect.

## **7.1 Metrics Aggregation**

Job runs hourly/daily:

```pseudo
SELECT * FROM exp.events WHERE created_at BETWEEN window
→ aggregate metrics by experiment, variant, metric_name
→ upsert into exp.metric_aggregates
```

## **7.2 Training Dataset Generation**

Described in detail in section 9.  
Outputs to `/datasets/{model_name}/{date}/training.parquet`.

## **7.3 Offline Replay Evaluation**

Described in detail in section 10.

---

# **8. MLflow Usage**

MLflow is used for:

- Tracking training runs  
- Storing metrics/loss curves  
- Registering model versions  
- Staging/promotion of models  

### **Minimum Logging Requirements**

- `params`: LR, batch size, epochs, dataset version  
- `metrics`: validation loss, offline eval score  
- `tags`: `policy_id`, `dataset_version`, `task`  
- `artefacts`: model weights, tokenizer, config  

### **Model Registration**

Example:

```text
Model name: planner_model
Version: 3
Tags: {"policy_id": "planner_policy", "stage": "candidate"}
```

Policies reference MLflow entries via:

- `mlflow_model_name`
- `mlflow_model_version`

---

# **9. Training & Fine-Tuning Workflow**

Developers must understand how to produce datasets and train models.

## **9.1 Dataset Requirements**

Each row includes:

- **Input context**  
  Household, pantry, constraints, workflow type, previous plan, mode.

- **Target output**  
  Accepted plan, edits applied by user.

- **Metadata**  
  timestamp, policy_version_id.

## **9.2 Pipeline Steps (Pseudo-code)**

```pseudo
1. Read exp.events for plan generation + acceptance.
2. Join with household/pantry/recipe/constraint domain tables.
3. Construct input and target fields.
4. Write to parquet under /datasets/planner/YYYY-MM-DD/
5. Log dataset version in MLflow tags.
```

## **9.3 Training Job (Conceptual)**

```pseudo
python train_planner.py \
  --dataset=/datasets/planner/2025-01-03 \
  --model=llama-3-8b \
  --epochs=3
```

Training script:

```pseudo
- load dataset
- load base model
- fine-tune (LoRA or QLoRA)
- log metrics + params to MLflow
- save artefacts → MLflow
- register new model version
```

---

# **10. Offline Evaluation Workflow**

Before any model is considered for live traffic, an offline replay job must be run.

### **Replay Steps (Pseudo-code)**

```pseudo
1. load dataset D
2. load model M (from MLflow)
3. for each row:
      prediction = run_policy(M, row.input)
      compare prediction vs row.target
4. compute:
      - plan_alignment_score
      - constraint_respect_score
      - edit_distance_score
      - diversity/leftovers metrics
5. log metrics to MLflow
6. write results to exp.offline_replay_results
```

### **Acceptance Criteria (Minimum)**

- No constraint violations  
- Equal or better alignment score than baseline  
- Equal or better predicted edit distance  
- Stable or improved variety/leftover metrics  

When satisfied, set MLflow model tag:

```
"stage"="candidate"
```

---

# **11. Moving from Training to Deployment**

## **11.1 Create Policy Version**

After successful offline evaluation:

```json
{
  "policy_id": "planner_policy",
  "version": 4,
  "mlflow_model_name": "planner_model",
  "mlflow_model_version": "4",
  "config_defaults": {}
}
```

## **11.2 Create Experiment Variant**

Add to `exp.variants`:

```json
{
  "experiment_id": "planner_policy_exp",
  "name": "candidate_model",
  "allocation": 0.1,
  "config": { "policy_version_id": "uuid-policy-v4" }
}
```

## **11.3 Online Experiment Monitoring**

Monitor via Metabase:

- acceptance rate  
- number of edits  
- validator intervention rate  
- latency  
- cost  

If successful:

- Increase allocation  
- Promote MLflow model tag `"stage"="production"`  
- Optionally retire baseline variant  

---

# **12. Metabase Setup**

## **12.1 Required Dashboards**

- Experiment Overview  
- Variant Performance Comparison  
- Offline vs Online Evaluation Trends  
- Model Quality Over Time  
- Latency & Cost Analyses  

## **12.2 Required Models/Views**

- `exp.events`  
- `exp.metric_aggregates`  
- Joined view with `exp.policy_versions`  

Example helpful view:

```sql
CREATE VIEW exp.v_experiment_metrics AS
SELECT
  e.name AS experiment_name,
  v.name AS variant_name,
  m.metric_name,
  m.mean,
  m.bucket_start
FROM exp.metric_aggregates m
JOIN exp.variants v ON v.id = m.variant_id
JOIN exp.experiments e ON e.id = m.experiment_id;
```

---

# **13. Configuration Conventions**

## **13.1 experiments.example.yml**

```yaml
experiments:
  - name: planner_policy_exp
    unit_type: user
    variants:
      - name: control
        allocation: 0.5
        config:
          policy_version_id: "uuid-v1"
      - name: leftovers_v2
        allocation: 0.5
        config:
          policy_version_id: "uuid-v2"
          params:
            exploration_rate: 0.2
```

## **13.2 policies.example.yml**

```yaml
policies:
  - name: planner_policy
    versions:
      - version: 1
        mlflow_model_name: planner_model
        mlflow_model_version: 1
      - version: 2
        mlflow_model_name: planner_model
        mlflow_model_version: 3
```

---

# **14. Developer Onboarding Path**

Developers should follow this order:

1. **Read architecture.md**  
2. **Read data-model.md**  
3. **Read experiments.md**  
4. **Inspect assignment-service.md**  
5. **Inspect event-ingestion-service.md**  
6. **Read mlflow-guide.md**  
7. **Read training-workflow.md**  
8. **Read offline-evaluation.md**  
9. **Read model-promotion.md**  
10. Explore dashboards in Metabase  

This ensures understanding of both **online experimentation** and the **model lifecycle**.

---

# **15. Common Pitfalls & Guidance**

- **Experiment drift**  
  → Ensure experiments are versioned and assignments immutable.

- **Invalid or incomplete events**  
  → Always include experiment + variant identifiers.

- **Using unregistered models**  
  → Policies must reference MLflow model versions only.

- **Mismatch between training dataset inputs and planner prompts**  
  → Training pipeline must mimic real request shape.

- **Premature traffic allocation**  
  → Always begin with ≤10%.

---

# **16. Summary**

This onboarding specification provides:

- Architecture  
- Data model  
- Experiment system  
- Event logging  
- Offline dataset pipelines  
- Training and fine-tuning process  
- Offline and online evaluation lifecycles  
- Model registration and promotion  
- Infrastructure expectations  
- Developer onboarding path  

Senior engineers can use this template to understand the shape of experimentation and model iteration, even without prior MLOps experience, and begin implementing a system capable of safe, structured ML-driven feature development.